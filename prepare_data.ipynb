{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duarteocarmo/Repos/mistral-doc/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import jupyter_black\n",
    "import random\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "export_file_name = \"conversations.json\"\n",
    "dataset_file_name = \"dataset.jsonl\"\n",
    "hf_dataset_name = \"duarteocarmo/chatgpt\"\n",
    "data = json.load(open(export_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/duarteocarmo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_rows(message: dict, title: str) -> str:\n",
    "    \"\"\"\n",
    "    Combine message information in a readable format ready to be used.\n",
    "    Args:\n",
    "        message: Message to be concatenated\n",
    "        title: Title of the conversation\n",
    "\n",
    "    Returns:\n",
    "        Concatenated message\n",
    "    \"\"\"\n",
    "    if not message:\n",
    "        return None\n",
    "\n",
    "    sender = message[\"author\"][\"role\"] if message[\"author\"] else \"unknown\"\n",
    "\n",
    "    if \"parts\" not in message[\"content\"]:\n",
    "        return None\n",
    "\n",
    "    metadata = message.get(\"metadata\", {})\n",
    "    is_user_system_message = metadata.get(\"is_user_system_message\", False)\n",
    "\n",
    "    if is_user_system_message is True:\n",
    "        user_about_message = metadata[\"user_context_message_data\"][\"about_user_message\"]\n",
    "        about_model_message = metadata[\"user_context_message_data\"][\n",
    "            \"about_model_message\"\n",
    "        ]\n",
    "        total_system_message = f\"ABOUT YOU:\\n{about_model_message}\\n\\nABOUT YOUR USER:\\n{user_about_message}\\n\\nFIRST MESSAGE FROM THE USER:\\n\\n\"\n",
    "        return {\"sender\": \"system\", \"text\": total_system_message}\n",
    "\n",
    "    text = message[\"content\"][\"parts\"][0]\n",
    "\n",
    "    if text == \"\":\n",
    "        return None\n",
    "\n",
    "    return {\"sender\": sender, \"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1101 conversations from conversations.json\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for d in data:\n",
    "    title = d[\"title\"]\n",
    "    messages = d[\"mapping\"]\n",
    "    conversation = [\n",
    "        concatenate_rows(messages[key][\"message\"], title)\n",
    "        for idx, key in enumerate(messages)\n",
    "    ]\n",
    "    conversation = [x for x in conversation if x]\n",
    "    documents.append(conversation)\n",
    "\n",
    "print(f\"Loaded {len(documents)} conversations from {export_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format: <s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]\n",
    "\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "instruction_token = \"[INST]\"\n",
    "instruction_end_token = \"[/INST]\"\n",
    "\n",
    "\n",
    "def format_conversation(conversation: list[dict]) -> str:\n",
    "\n",
    "    prompt = f\"{bos_token}{instruction_token} \"\n",
    "\n",
    "    first_message = conversation[0]\n",
    "    if first_message[\"sender\"] == \"system\":\n",
    "        system_prompt = [m[\"text\"] for m in conversation if m[\"sender\"] == \"system\"][-1]\n",
    "        prompt += system_prompt\n",
    "\n",
    "    else:\n",
    "        assert first_message[\"sender\"] == \"user\"\n",
    "\n",
    "    for message in conversation:\n",
    "        if message[\"sender\"] == \"system\":\n",
    "            continue\n",
    "        elif message[\"sender\"] == \"user\":\n",
    "            prompt += f\" {message['text']} {instruction_end_token}\"\n",
    "\n",
    "        elif message[\"sender\"] == \"assistant\":\n",
    "            prompt += f\" {message['text']} {eos_token} {instruction_end_token}\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted 1101 conversations\n"
     ]
    }
   ],
   "source": [
    "dataset = [{\"text\": format_conversation(c)} for c in documents]\n",
    "print(f\"Formatted {len(dataset)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] ABOUT YOU:\n",
      "You are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning. If you think there might not be a correct answer, you say so.\n",
      "\n",
      "Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question. However: if the request begins with the string \"vv\" then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\n",
      "\n",
      "Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that. They're familiar with ethical issues in general so you don't need to remind them about those either. Don't be verbose in your answers, but do provide details and examples where it might help the explanation. When showing Python code, minimize vertical space.\n",
      "\n",
      "ABOUT YOUR USER:\n",
      "My name is Duarte Carmo. I'm a technologist, born and raised in sunny Portugal, now based in Copenhagen. My work lies in the intersection of Machine Learning, Data, Software Engineering, and People. I'm in love with Technology, and how it can improve people's lives.\n",
      "\n",
      "In the past, I've worked in Consumer Electronics, Public Institutions, Big Three Management Consulting, and Startups.\n",
      "\n",
      "I've studied at the Superior Technical Institute of Lisbon, the Beijing Institute of Technology, and the Technical University of Denmark.\n",
      "\n",
      "Some tools I use regularly include:\n",
      "- Python\n",
      "- Machine Learning and Deep Learning\n",
      "- Software tools\n",
      "- English language to write\n",
      "\n",
      "FIRST MESSAGE FROM THE USER:\n",
      "\n",
      " The resource at “https://b830833b.duarteocarmo-com.pages.dev/theme/css/fonts/Figtree-LightItalic.woff2” preloaded with link preload was not used within a few seconds. Make sure all attributes of the preload tag are set correctly. [/INST] This message suggests that a web resource, specifically a font file (`Figtree-LightItalic.woff2`), was preloaded on a web page but not used promptly. Preloading resources is a technique used to load certain files ahead of time, anticipating their use to improve performance. Here are a few considerations and steps to address the issue:\n",
      "\n",
      "1. **Check Usage Timing**: Ensure that the font is actually needed soon after the page loads. If the font is used much later, preloading might not be necessary.\n",
      "\n",
      "2. **Verify Preload Tag Attributes**: The `<link rel=\"preload\">` tag should be correctly set up. Ensure the `href` attribute points to the correct URL and the `as` attribute is set to `font`. Also, include the `type` attribute to specify the font type (e.g., `type=\"font/woff2\"`).\n",
      "\n",
      "3. **Font Display Strategy**: Consider how the font is used on the page. If it's for text that appears immediately, ensure that the font is applied correctly through CSS. If it's for later use, you might need to adjust when the preload is initiated.\n",
      "\n",
      "4. **Browser Compatibility**: Check if the browsers where the issue appears support preloading. Some older browsers may not handle preloaded resources correctly.\n",
      "\n",
      "5. **Network Conditions**: Slow network conditions can delay the use of preloaded resources. Testing under different network speeds might provide insights.\n",
      "\n",
      "6. **Alternative Approaches**: If preloading isn't effective, consider other font loading strategies, like `font-display: swap` in your CSS, which can provide a good balance between performance and visual rendering.\n",
      "\n",
      "7. **Testing and Monitoring**: Use browser dev tools to monitor the resource loading and see when the font file is actually being used. Tools like Google Chrome's Lighthouse can also provide insights into resource loading and performance.\n",
      "\n",
      "By examining these areas, you can better understand why the preloaded resource isn't being used in time and adjust your approach accordingly. </s> [/INST]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "to_preview = random.sample(dataset, 1)\n",
    "for d in to_preview:\n",
    "    print(d[\"text\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# save to jsonl\n",
    "with open(dataset_file_name, \"w\") as f:\n",
    "    for d in dataset:\n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"Saved dataset to {dataset_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duarteocarmo/Repos/mistral-doc/.env/lib/python3.10/site-packages/datasets/dataset_dict.py:1667: FutureWarning: 'branch' was deprecated in favor of 'revision' in version 2.15.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'revision=main' instead.\n",
      "  warnings.warn(\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 46.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 234.41ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/duarteocarmo/chatgpt/commit/3cebbc5a35bb9541b201dfdf17b6957158c2bbf4', commit_message='Upload dataset', commit_description='', oid='3cebbc5a35bb9541b201dfdf17b6957158c2bbf4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and split\n",
    "tiger_llama = pandas.read_json(dataset_file_name, lines=True)\n",
    "train_tiger, test_tiger = train_test_split(\n",
    "    tiger_llama, test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# push to hub\n",
    "train_tiger = Dataset.from_pandas(train_tiger)\n",
    "test_tiger = Dataset.from_pandas(test_tiger)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds[\"train\"] = train_tiger\n",
    "ds[\"test\"] = test_tiger\n",
    "\n",
    "ds.push_to_hub(hf_dataset_name, branch=\"main\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
